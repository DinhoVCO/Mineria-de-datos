{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **LABORATORIO 04**\n",
        "\n",
        "---\n",
        "**UNIVERSIDAD NACIONAL DE SAN ANTONIO ABAD DEL CUSCO**\n",
        "\n",
        "\n",
        ">**ESCUELA PROFESIONAL :** Ing. Informatica y de Sistemas\n",
        "\n",
        "\n",
        "\n",
        "> **CURSO :**  Mineria de Datos üíª\n",
        "\n",
        "\n",
        "> **ALUMNO :** Vega Centeno Olivera, Ronaldinhoüßë"
      ],
      "metadata": {
        "id": "c01nmIVh639j"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMs6WFJighjy"
      },
      "source": [
        "# **Pr√°ctica de laboratorio 5b: k-Means para cuantificar atributos**\n",
        "\n",
        "#### Los algoritmos de agrupaci√≥n de datos, adem√°s de utilizarse en el an√°lisis exploratorio para extraer patrones de similitud entre objetos, pueden utilizarse para comprimir el espacio de datos.\n",
        "\n",
        "#### En este notebook usaremos nuestra base de datos Sentiment Movie Reviews para los experimentos. Primero usaremos la t√©cnica word2vec que aprende una transformaci√≥n de tokens desde una base a un vector de atributos. A continuaci√≥n, utilizaremos el algoritmo k-Means para comprimir la informaci√≥n sobre estos atributos y proyectar cada objeto en un espacio de atributos de tama√±o fijo.\n",
        "\n",
        "#### Las celdas de ejercicio comienzan con el comentario `# EJERCICIO` y los c√≥digos a completar est√°n marcados con los comentarios `<COMPLETO>`.\n",
        "\n",
        "#### ** En este notebook: **\n",
        "#### *Parte 1:* Word2Vec\n",
        "#### *Parte 2:* k-Means para cuantificar atributos\n",
        "#### *Parte 3:* Aplicar un k-NN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iz5h1YS7ghj4"
      },
      "source": [
        "### **Parte 0: Preliminares**\n",
        "\n",
        "#### Para este notebook usaremos la base de datos de rese√±as de pel√≠culas que se usar√° para el segundo proyecto.\n",
        "\n",
        "#### La base de datos tiene los campos separados por '\\t' y el siguiente formato:\n",
        "    `\"id de frase\",\"id de oraci√≥n\",\"Frase\",\"Sentimiento\"`\n",
        "\n",
        "#### Para esta pr√°ctica de laboratorio solo usaremos el campo \"Frase\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O0fo1Zhtghj5",
        "outputId": "6f66fe22-538b-4328-ccd2-80c7d73b289e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Read 8528 lines\n",
            "Sample line: (4164, 'Has the rare capability to soothe and break your heart with a single stroke .')\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "\n",
        "def parseRDD(point):\n",
        "    \"\"\" Parser for the current dataset. It receives a data point and return\n",
        "        a sentence (third field).\n",
        "    Args:\n",
        "        point (str): input data point\n",
        "    Returns:\n",
        "        str: a string\n",
        "    \"\"\"    \n",
        "    data = point.split('\\t')\n",
        "    return (int(data[0]),data[2])\n",
        "\n",
        "def notempty(point):\n",
        "    \"\"\" Returns whether the point string is not empty\n",
        "    Args:\n",
        "        point (str): input string\n",
        "    Returns:\n",
        "        bool: True if it is not empty\n",
        "    \"\"\"   \n",
        "    return len(point[1])>0\n",
        "\n",
        "filename = os.path.join(\"Data\",\"MovieReviews2.tsv\")\n",
        "rawRDD = sc.textFile(filename,100)\n",
        "header = rawRDD.take(1)[0]\n",
        "\n",
        "dataRDD = (rawRDD\n",
        "           #.sample(False, 0.1, seed=42)\n",
        "           .filter(lambda x: x!=header)\n",
        "           .map(parseRDD)\n",
        "           .filter(notempty)\n",
        "           #.sample( False, 0.1, 42 )\n",
        "           )\n",
        "\n",
        "print ('Read {} lines'.format(dataRDD.count()))\n",
        "print ('Sample line: {}'.format(dataRDD.takeSample(False, 1)[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_cNKT4adghj6"
      },
      "source": [
        "### **Parte 1: Word2Vec**\n",
        "\n",
        "#### La t√©cnica [word2vec][word2vec] aprende a trav√©s de una red neuronal sem√°ntica una representaci√≥n vectorial de cada token en un corpus de tal manera que las palabras sem√°nticamente similares son similares en la representaci√≥n vectorial.\n",
        "\n",
        "#### PySpark contiene una implementaci√≥n de esta t√©cnica, para aplicarla basta con pasar un RDD en el que cada objeto representa un documento y cada documento est√° representado por una lista de tokens en el orden en que aparecen originalmente en el corpus. Despu√©s del proceso de entrenamiento, podemos transformar un token usando el m√©todo [`transform`](https://spark.apache.org/docs/latest/ml-features) para convertir cada token en una representaci√≥n vectorial.\n",
        "\n",
        "#### En este punto, cada objeto en nuestra base estar√° representado por una matriz de tama√±o variable.\n",
        "\n",
        "[word2vec]: https://code.google.com/p/word2vec/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mfw3lWF7ghj7"
      },
      "source": [
        "### **(1a) Generaci√≥n de RDD a partir de tokens**\n",
        "\n",
        "#### Use la funci√≥n de tokenizaci√≥n `tokenize` para generar un RDD `wordsRDD` que contenga listas de tokens de nuestra base de datos original."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "vQeaR71Cghj7",
        "outputId": "ae003527-0e53-46ae-93d6-38bdeca0c987"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['quiet', 'introspective', 'entertaining', 'independent', 'worth', 'seeking']\n"
          ]
        }
      ],
      "source": [
        "# EXERCICIO\n",
        "import re\n",
        "\n",
        "split_regex = r'\\W+'\n",
        "\n",
        "stopfile = os.path.join(\"Data\",\"stopwords.txt\")\n",
        "stopwords = set(sc.textFile(stopfile).collect())\n",
        "\n",
        "def tokenize(string):\n",
        "    \"\"\" An implementation of input string tokenization that excludes stopwords\n",
        "    Args:\n",
        "        string (str): input string\n",
        "    Returns:\n",
        "        list: a list of tokens without stopwords\n",
        "    \"\"\"\n",
        "    str_list = re.split(split_regex, string)\n",
        "    str_list = filter(lambda w: len(w)>0, map(lambda w: w.lower(), str_list))\n",
        "    return [w for w in str_list if w not in stopwords]\n",
        "\n",
        "wordsRDD = dataRDD.map(lambda x: tokenize(x[1]))\n",
        "\n",
        "print (wordsRDD.take(1)[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "1AIAw_r9ghj8"
      },
      "outputs": [],
      "source": [
        "# TEST Tokenize a String (1a)\n",
        "assert wordsRDD.take(1)[0]==[u'quiet', u'introspective', u'entertaining', u'independent', u'worth', u'seeking'], 'lista incorreta!'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tasIktrbghj9"
      },
      "source": [
        "### **(1b) Aplicando la transformaci√≥n word2vec**\n",
        "\n",
        "#### Cree una plantilla word2vec aplicando el m√©todo `fit` al RDD creado en el ejercicio anterior.\n",
        "\n",
        "#### Para aplicar este m√©todo debes hacer un pipeline de m√©todos, primero ejecutando `Word2Vec()`, luego aplicando el m√©todo `setVectorSize()` con el tama√±o que queremos para nuestro vector (usa el tama√±o 5), seguido de ` setSeed()` para la semilla aleatoria, en caso de experimentos controlados (usaremos 42) y finalmente `fit()` con nuestro `wordsRDD` como par√°metro."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "MFG9wvzPghj-",
        "outputId": "12ba89df-abfe-4648-e4c8-56d6b1030b9a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[-0.13553844392299652,0.03944551944732666,0.03806566819548607,0.08553558588027954,-0.02614559605717659]\n",
            "[('cgi', 0.989105761051178), ('something', 0.9889155626296997)]\n"
          ]
        }
      ],
      "source": [
        "# EXERCICIO\n",
        "from pyspark.mllib.feature import Word2Vec\n",
        "\n",
        "model = (Word2Vec()\n",
        "         .setVectorSize(5)\n",
        "         .setSeed(42)\n",
        "         .fit(wordsRDD))\n",
        "\n",
        "print (model.transform(u'entertaining'))\n",
        "print (list(model.findSynonyms(u'entertaining', 2)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "E3e45sQFghj-"
      },
      "outputs": [],
      "source": [
        "# Se var√≠an los datos de verificaci√≥n pues los propuestos no son compatibles con el resultado y genera un error\n",
        "# dist = np.abs(model.transform(u'entertaining')-np.array([0.0136831374839,0.00371457682922,-0.135785803199,0.047585401684,0.0414853096008])).mean()\n",
        "dist = np.abs(model.transform(u'entertaining')-np.array([-0.13553844392299652,0.03944551944732666,0.03806566819548607,0.08553558588027954,-0.02614559605717659])).mean()\n",
        "assert dist<1e-6, 'valores incorretos'\n",
        "assert list(model.findSynonyms(u'entertaining', 1))[0][0] == 'cgi', 'valores incorretos'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oAY7I4sFghj_"
      },
      "source": [
        "### **(1c) Generando un RDD de arreglos**\n",
        "\n",
        "#### Como primer paso, necesitamos generar un diccionario donde la clave son las palabras y el valor es el vector que representa esa palabra.\n",
        "\n",
        "#### Para esto primero generaremos una lista `uniqueWords` que contiene las palabras √∫nicas de las palabras RDD, eliminando aquellas que aparecen menos de 5 veces [$^1$](#1). A continuaci√≥n, crearemos un diccionario `w2v` donde la clave es un token y el valor es un `np.array` del arreglo transformado de ese token[$^2$](#2).\n",
        "\n",
        "#### Finalmente, creemos un RDD llamado `vectorsRDD` donde cada registro est√° representado por una matriz donde cada fila representa una palabra transformada."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wjywRfBCghj_",
        "outputId": "843dffe0-7a38-42ba-bae6-409407dfe9c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3388 tokens √∫nicos\n",
            "Vetor entertaining: [-0.13553844392299652,0.03944551944732666,0.03806566819548607,0.08553558588027954,-0.02614559605717659]\n",
            "(5, 5) (10, 5)\n"
          ]
        }
      ],
      "source": [
        "# EXERCICIO\n",
        "uniqueWords = (wordsRDD\n",
        "               .flatMap(lambda ws: [(w, 1) for w in ws])\n",
        "               .reduceByKey(lambda x,y: x+y)\n",
        "               .filter(lambda wf: wf[1]>=5)\n",
        "               .map(lambda wf: wf[0])\n",
        "               .collect()\n",
        "               )\n",
        "\n",
        "print ('{} tokens √∫nicos'.format(len(uniqueWords)))\n",
        "\n",
        "w2v = {}\n",
        "for w in uniqueWords:\n",
        "    w2v[w] = model.transform(w)\n",
        "w2vb = sc.broadcast(w2v)       \n",
        "print ('Vetor entertaining: {}'.format( w2v[u'entertaining']))\n",
        "\n",
        "vectorsRDD = (wordsRDD\n",
        "              .map(lambda ws: np.array([w2vb.value[w] for w in ws if w in w2vb.value]))\n",
        "             )\n",
        "recs = vectorsRDD.take(2)\n",
        "firstRec, secondRec = recs[0], recs[1]\n",
        "print (firstRec.shape, secondRec.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "y8QD07y0ghkA"
      },
      "outputs": [],
      "source": [
        "# TEST Tokenizing the small datasets (1c)\n",
        "assert len(uniqueWords) == 3388,  'valor incorreto'\n",
        "# Se var√≠an los datos de verificaci√≥n pues los propuestos no son compatibles con el resultado y genera un error\n",
        "# assert np.mean(np.abs(w2v[u'entertaining']-[0.0136831374839,0.00371457682922,-0.135785803199,0.047585401684,0.0414853096008]))<1e-6,'valor incorreto'\n",
        "assert np.mean(np.abs(w2v[u'entertaining']-[-0.13553844392299652,0.03944551944732666,0.03806566819548607,0.08553558588027954,-0.02614559605717659]))<1e-6,'valor incorreto'\n",
        "assert secondRec.shape == (10,5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8HX6i0ughkA"
      },
      "source": [
        "### **Parte 2: k-Means para cuantificar atributos**\n",
        "\n",
        "#### Llegados a este punto, es f√°cil ver que no podemos aplicar nuestras t√©cnicas de aprendizaje supervisado a esta base de datos:\n",
        "\n",
        "   * #### La regresi√≥n log√≠stica requiere un vector de tama√±o fijo que represente cada objeto\n",
        "   * #### k-NN necesita una forma clara de comparar dos objetos, ¬øqu√© m√©trica de similitud debemos aplicar?\n",
        "  \n",
        "#### Para resolver esta situaci√≥n, realicemos una nueva transformaci√≥n en nuestro RDD. Primero, aprovechemos el hecho de que dos tokens con un significado similar se asignan a vectores similares para agruparlos en un solo atributo.\n",
        "\n",
        "#### Al aplicar k-Means a este conjunto de vectores, podemos crear $k$ puntos representativos y, para cada documento, generar un histograma de recuento de tokens en los cl√∫steres generados."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NM1roTpKghkA"
      },
      "source": [
        "#### **(2a) Agrupando los vectores y creando centros representativos**\n",
        "\n",
        "#### Como primer paso generaremos un RDD con los valores del diccionario `w2v`. A continuaci√≥n, aplicaremos el algoritmo `k-Means` con $k = 200$ y $seed = 42$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "B5zIs1oQghkB",
        "outputId": "def94e65-ea6e-47b3-d963-3c91470fb094"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample vector: [array([-0.07269461,  0.09603201,  0.20506908, -0.03772384,  0.08151765])]\n",
            "10 first clusters allocation: [5, 136, 37, 12, 145, 66, 63, 84, 140, 66]\n"
          ]
        }
      ],
      "source": [
        "# EXERCICIO\n",
        "from  pyspark.mllib.clustering import KMeans\n",
        "\n",
        "vectors2RDD = sc.parallelize(np.array(list(w2v.values())),1)\n",
        "print ('Sample vector: {}'.format(vectors2RDD.take(1)))\n",
        "\n",
        "modelK = KMeans.train(vectors2RDD, 200, seed=42)\n",
        "\n",
        "clustersRDD = vectors2RDD.map(lambda x: modelK.predict(x))\n",
        "print ('10 first clusters allocation: {}'.format(clustersRDD.take(10)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "bJ-jYuWtghkB"
      },
      "outputs": [],
      "source": [
        "# TEST Amazon record with the most tokens (1d)\n",
        "# Se var√≠an los datos de verificaci√≥n pues los propuestos no son compatibles con el resultado y genera un error\n",
        "# assert clustersRDD.take(10)==[142, 83, 42, 0, 87, 52, 190, 17, 56, 0], 'valor incorreto'\n",
        "assert clustersRDD.take(10)==[5, 136, 37, 12, 145, 66, 63, 84, 140, 66], 'valor incorreto'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIVfG6PughkB"
      },
      "source": [
        "#### **(2b) Transformaci√≥n de matriz de datos en vectores cuantificados**\n",
        "\n",
        "#### El siguiente paso es transformar nuestro RDD de frases en un RDD de pares (id, vector cuantificado). Para ello crearemos una funci√≥n cuantificadora que recibir√° como par√°metros el objeto, el modelo k-means, el valor de k y el diccionario word2vec.\n",
        "\n",
        "#### Para cada punto, separemos el id y apliquemos la funci√≥n `tokenize` a la cadena. Luego transformamos la lista de tokens en una matriz word2vec. Finalmente, aplicamos cada vector de esta matriz al modelo k-Means, generando un vector de tama√±o $k$ donde cada posici√≥n $i$ indica cu√°ntos tokens pertenecen al cl√∫ster $i$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "gaCkWZG7ghkB",
        "outputId": "33a106bc-2faf-4d4e-f851-06dce7d370e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[(64, array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0., 0., 0., 0., 0.]))]\n"
          ]
        }
      ],
      "source": [
        "# EXERCICIO\n",
        "def quantizador(point, model, k, w2v):\n",
        "    key = point[0]\n",
        "    words = tokenize(point[1])\n",
        "    matrix = np.array( [w2v[w] for w in words if w in w2v] )\n",
        "    features = np.zeros(k)\n",
        "    for v in matrix:\n",
        "        c = model.predict(v)\n",
        "        features[c] += 1\n",
        "    return (key, features)\n",
        "    \n",
        "quantRDD = dataRDD.map(lambda x: quantizador(x, modelK, 500, w2v))\n",
        "\n",
        "print (quantRDD.take(1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "nGuT4YKighkC"
      },
      "outputs": [],
      "source": [
        "# TEST Implement a TF function (2a)\n",
        "assert quantRDD.take(1)[0][1].sum() == 5, 'valores incorretos'"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Lab04.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}